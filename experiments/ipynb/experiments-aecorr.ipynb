{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import operator as op\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as trans\n",
    "import more_trans\n",
    "import more_sampler\n",
    "import exprlib\n",
    "import trainlib\n",
    "import vmdata\n",
    "from aecorr.trainutils import tbatch2cbatch\n",
    "import aecorr.eval\n",
    "import aecorr.models.unet_pred3_f1to2 as net_module\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "rundir = os.path.expandvars('$PROJ_HOME/experiments/src/aecorr')\n",
    "todir = os.path.expandvars('$PROJ_HOME/experiments/ipynb/data.experiments-aecorr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net_module.Autoencoder()\n",
    "trainlib.load_checkpoint(net, os.path.join(rundir, 'run.1', 'save'), 'checkpoint_{}_{}.pth', (0, 29990))\n",
    "net.eval()\n",
    "for p in net.parameters():\n",
    "    p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = vmdata.dataset_root(1, 8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmlstat = vmdata.get_normalization_stats(root, bw=True)\n",
    "normalize = trans.Normalize(*nmlstat)\n",
    "denormalize = more_trans.DeNormalize(*nmlstat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=(0.5116854310035706,), std=(0.07510992884635925,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeNormalize(mean=(0.5116854310035706,), std=(0.07510992884635925,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module more_trans:\n",
      "\n",
      "__init__(self, normalize:torchvision.transforms.transforms.Normalize, pool_scale:int=1, downsample_scale:int=1, to_rgb:bool=False)\n",
      "    :param normalize: the normalization transform\n",
      "    :param pool_scale: the overall scale of the pooling operations in\n",
      "           subsequent encoder; the image will be cropped to (H', W') where\n",
      "           H' and W' are the nearest positive integers to H and W that are\n",
      "           the power of ``pool_scale``, so that ``unpool(pool(x))`` is of\n",
      "           the same shape as ``x``\n",
      "    :param downsample_scale: the scale to downsample the video frames\n",
      "    :param to_rgb: if True, at the last step convert from B&W image to\n",
      "           RGB image\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(more_trans.BWCAEPreprocess.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module more_sampler:\n",
      "\n",
      "__init__(self, indices, window_width:int, shuffled:bool=False, batch_size:int=1, drop_last:bool=False)\n",
      "    :param indices: array-like integer indices to sample; when presented as\n",
      "           a list of arrays, no sample will span across more than one array\n",
      "    :param window_width: the width of the window; if ``window_width`` is\n",
      "           larger than the length of ``indices`` or the length of one of\n",
      "           the sublists, then that list won't be sampled\n",
      "    :param shuffled: whether to shuffle sampling, but the indices order\n",
      "           within a batch is never shuffled\n",
      "    :param batch_size: how many batches to yield upon each sampling\n",
      "    :param drop_last: True to drop the remaining batches if the number of\n",
      "           remaining batches is less than ``batch_size``\n",
      "    \n",
      "    Note on ``batch_size``\n",
      "    ----------------------\n",
      "    \n",
      "    When ``batch_size = 2``, assuming that the two batch of indices are\n",
      "    ``[1, 2, 3, 4]`` and ``[4, 5, 6, 7]``, then the yielded hyper-batch\n",
      "    will be ``[1, 2, 3, 4, 4, 5, 6, 7]``.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(more_sampler.SlidingWindowBatchSampler.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = more_trans.BWCAEPreprocess(normalize, net_module.pool_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalpred_it(root, transform, detransform, indices):\n",
    "    with vmdata.VideoDataset(root, transform=transform) as vdset:\n",
    "        sam = more_sampler.SlidingWindowBatchSampler(indices, 3)\n",
    "        for frames in DataLoader(vdset, batch_sampler=sam):\n",
    "            frames = more_trans.rearrange_temporal_batch(frames, 3)\n",
    "            inputs, targets = frames[:,:,[0,2],...], frames[:,:,[1],...]\n",
    "            inputs, targets = tbatch2cbatch(inputs), tbatch2cbatch(targets)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            inputs = inputs.detach().transpose(0, 1)\n",
    "            prediction = outputs.detach().squeeze(0)\n",
    "            target = targets.detach().squeeze(0)\n",
    "            inputs = list(map(op.methodcaller('numpy'),\n",
    "                              map(partial(torch.clamp, min=0.0, max=1.0),\n",
    "                                  map(detransform, inputs))))\n",
    "            prediction = torch.clamp(detransform(prediction), 0.0, 1.0).numpy()\n",
    "            target = torch.clamp(detransform(target), 0.0, 1.0).numpy()\n",
    "            yield inputs[0][0], target[0], inputs[1][0], prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "_eit = evalpred_it(root, transform, denormalize, range(500, 510))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(4):\n",
    "    f1, f2, f3, pf2 = next(_eit)\n",
    "f = f1, f2, f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d76e0af331240288328a9bee536fcab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', max=2), Checkbox(value=False, description='showp'), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def showf(i, showp):\n",
    "    if not showp:\n",
    "        plt.imshow(f[i], cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(pf2, cmap='gray')\n",
    "\n",
    "interact(showf, i=widgets.IntSlider(min=0,max=2,step=1,value=0), showp=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalcorr_it(root, transform, detransform, indices):\n",
    "    with vmdata.VideoDataset(root, transform=transform) as vdset:\n",
    "        sam = more_sampler.SlidingWindowBatchSampler(indices, 3)\n",
    "        for frames in DataLoader(vdset, batch_sampler=sam):\n",
    "            frames = more_trans.rearrange_temporal_batch(frames, 3)\n",
    "            inputs, targets = frames[:,:,[0,2],...], frames[:,:,[1],...]\n",
    "            inputs, targets = tbatch2cbatch(inputs), tbatch2cbatch(targets)\n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            inputs = inputs.detach().transpose(0, 1)\n",
    "            prediction = outputs.detach().squeeze(0)\n",
    "            target = targets.detach().squeeze(0)\n",
    "            inputs = list(map(op.methodcaller('numpy'),\n",
    "                              map(partial(torch.clamp, min=0.0, max=1.0),\n",
    "                                  map(detransform, inputs))))\n",
    "            prediction = torch.clamp(detransform(prediction), 0.0, 1.0).numpy()\n",
    "            target = torch.clamp(detransform(target), 0.0, 1.0).numpy()\n",
    "            yield inputs[0][0], target[0], inputs[1][0], prediction[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
